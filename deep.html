<!DOCTYPE HTML>
<html>

<head>
    <title>Machine Learning</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="assets1/css/main.css">
    <noscript>
        <link rel="stylesheet" href="assets1/css/noscript.css" /></noscript>
</head>

<body class="is-preload">

    <!-- Wrapper -->
    <div id="wrapper">

        <!-- Header -->
        <header id="header">
            <h1><a href="#footer"><strong>Deep Learning Projects</strong> </a></h1>
            <nav>
                <ul>
                    <li><a href="index.html" class="icon solid fa-info-circle">GoBack</a></li>
                </ul>
            </nav>
        </header>

        <!-- Main -->
        <div id="main">
            <article class="thumb">
                <a href="https://miro.medium.com/max/6592/1*2G4GdnBQW5bcjJx4rSuZxg.gif" class="image"><img
                        src="https://miro.medium.com/proxy/1*pO5X2c28F1ysJhwnmPsy3Q.gif" alt="" /></a>
                <li><a href="https://github.com/Ajayprr/3_Crash-Course-On-Multi-Layer-Perceptron-Neural-Networks"
                        class="icon brands alt fa-github"><span class="label">GitHub Code</span></a></li>
                <h2>Multi-Layer Perceptron Neural Networks</h2>
                <p>Crash course in the terminology and processes used in the field of multi-layer perceptron artificial neural
                    networks.<br>The building blocks of neural networks including neurons, weights and activation functions.
                    How the building blocks are used in layers to create networks.<br>
                    How networks are trained from example data.</p>
            </article>
            <article class="thumb">
                <a href="https://adatis.co.uk/wp-content/uploads/ANN-Graph.gif" class="image"><img
                        src="https://beta.techcrunch.com/wp-content/uploads/2017/04/neural_networks_convolution_layers_gumgum1.gif" alt="" /></a>
                <li><a href="https://github.com/Ajayprr/1_life_cycle_neural_network_models_in_keras"
                        class="icon brands alt fa-github"><span class="label">GitHub Code</span></a></li>
                <h2> Life cycles of neural network models</h2>
                <p>Discovering the step-by-step life-cycle for creating, training and evaluating deep learning neural networks in Keras and
                how to make predictions with a trained model.</p>
                
            </article>
            <article class="thumb">
                <a href="https://miro.medium.com/max/1052/1*GcI7G-JLAQiEoCON7xFbhg.gif"
                    class="image"><img src="https://miro.medium.com/max/790/1*nYf_cUIHFEWU1JXGwnz-Ig.gif" alt="" /></a>
                    <li><a href="https://github.com/Ajayprr/2_Convolutional-Neural-Networks-for-Machine-Learning"
                            class="icon brands alt fa-github"><span class="label">GitHub Code</span></a></li>
                <h2>Introduction to Convolutional neural networks</h2>
                <p>Discovering Convolutional Neural Networks for deep learning, also called ConvNets or CNNs.</p>
            </article>
            
            <article class="thumb">
                <a href="https://cdn-images-1.medium.com/max/1000/1*bdUqInFvt5N4ytJfVSI84A.gif" class="image"><img
                        src="https://cdn-images-1.medium.com/max/1000/1*TqcA9EIUF-DGGTBhIx_qbQ.gif"
                        alt="" /></a>
                        <li><a href="https://github.com/Ajayprr/1_life_cycle_neural_network_models_in_keras"
                                class="icon brands alt fa-github"><span class="label">GitHub Code</span></a></li>
                <h2>Recurrent Neural Networks</h2>
                <p>The limitations of Multilayer Perceptrons that are addressed by recurrent neural networks.<br>
                The problems that must be addressed to make Recurrent Neural networks useful.<br>
                The details of the Long Short-Term Memory networks used in applied deep learning.</p>
            </article>
            <article class="thumb">
                <a href="https://miro.medium.com/max/960/1*nuoD-tQylhMj-SF8WJfUjg.gif" class="image"><img
                        src="https://i.pinimg.com/originals/c7/37/a4/c737a4b78935d45b9928aea9c9ecf4b6.gif" alt="" /></a>
                        <li><a href="https://github.com/Ajayprr/1_life_cycle_neural_network_models_in_keras"
                                class="icon brands alt fa-github"><span class="label">GitHub Code</span></a></li>
                <h2>Visualize the performance of Deep learning model</h2>
                <p>In this post you will discover how you can review and visualize the performance of deep learning models over time during
                training in Python with Keras.</p>
            </article>
            <article class="thumb">
                <a href="http://carat.st.bv.tum.de/caratuserswiki/images/Regularization_lapalce_hexa.gif" class="image"><img
                        src="https://cdn-images-1.medium.com/max/1000/1*Dcupj4ytCPTNrKV9knLZLw.gif" alt="" /></a>
                        <li><a href="https://github.com/Ajayprr/1_life_cycle_neural_network_models_in_keras"
                                class="icon brands alt fa-github"><span class="label">GitHub Code</span></a></li>
                <h2>Regularization in Deep Learning Models</h2>
                <p>How the dropout regularization technique works.<br>
                How to use dropout on your input layers.<br>
                How to use dropout on your hidden layers.<br>
                How to tune the dropout level on your problem..</p>
            </article>
            <article class="thumb">
                <a href="https://matthewrocklin.com/slides/images/fail-case.gif" class="image"><img
                        src="https://media3.giphy.com/media/3o6ZthgwwYimHPVTaM/source.gif"
                        alt="" /></a>
                        <li><a href="https://github.com/Ajayprr/1_life_cycle_neural_network_models_in_keras"
                                class="icon brands alt fa-github"><span class="label">GitHub Code</span></a></li>
                <h2 style="color:blue">Grid Search Hyperparameters for Deep Learning Models</h2>
                <p style="color:black">How to wrap Keras models for use in scikit-learn and how to use grid search.<br>
                How to grid search common neural network parameters such as learning rate, dropout rate, epochs and number of neurons.<br>
                How to define your own hyperparameter tuning experiments on your own projects.</p>
            </article>
            <article class="thumb">
                <a href="https://miro.medium.com/max/3890/1*ih6faUAZq9rhpo8sOFKYSw.gif" class="image"><img
                        src="https://thumbs.gfycat.com/SlushySlimGalapagosmockingbird-max-1mb.gif" alt="" /></a>
                        <li><a href="https://github.com/Ajayprr/1_life_cycle_neural_network_models_in_keras"
                                class="icon brands alt fa-github"><span class="label">GitHub Code</span></a></li>
                <h2 style="color:red;">
                    Handwritten Digit Recognition
                </h2>
                <p>How to load the MNIST dataset in Keras.
                How to develop and evaluate a baseline neural network model for the MNIST problem.<br>
                How to implement and evaluate a simple Convolutional Neural Network for MNIST.<br>
                How to implement a close to state-of-the-art deep learning model for MNIST.</p>
            </article>
            <article class="thumb">
                <a href="https://miro.medium.com/max/3840/1*rv-_-8LemZW6m9YrWBQx9w.gif" class="image"><img src="https://miro.medium.com/max/1575/1*Mi4TbX-_KoxJ3Y3giJBfXg.gif" alt="" /></a>
                <li><a href="https://github.com/Ajayprr/1_life_cycle_neural_network_models_in_keras"
                        class="icon brands alt fa-github"><span class="label">GitHub Code</span></a></li>
                <h2>Object Recognition with Convolutional Neural Networks</h2>
                <p>About the CIFAR-10 object recognition dataset and how to load and use it in Keras.<br>
                How to create a simple Convolutional Neural Network for object recognition.<br>
                How to lift performance by creating deeper Convolutional Neural Networks.</p>
            </article>
            <article class="thumb">
                <a href="https://thumbs.gfycat.com/LeadingEvilIndri-size_restricted.gif" class="image"><img
                        src="https://i.makeagif.com/media/6-28-2018/fIQgqU.gif"
                        alt="" /></a>
                        <li><a href="https://github.com/Ajayprr/1_life_cycle_neural_network_models_in_keras"
                                class="icon brands alt fa-github"><span class="label">GitHub Code</span></a></li>
                <h2>Predict Sentiment From Movie Reviews</h2>
                <p style="color:darkgreen;">About the IMDB sentiment analysis problem for natural language processing and how to load it in Keras.<br>
                How to use word embedding in Keras for natural language problems.<br>
                How to develop and evaluate a multi-layer perception model for the IMDB problem.<br>
                How to develop a one-dimensional convolutional neural network model for the IMDB problem.</p>
            </article>
            <article class="thumb">
                <a href="https://i.pinimg.com/originals/35/78/ba/3578bab0d217108930a655a862e1bae7.gif"
                    class="image"><img src="https://media1.giphy.com/media/xT0BKsuAQgI5u9SZMc/source.gif" alt="" /></a>
                    <li><a href="https://github.com/Ajayprr/1_life_cycle_neural_network_models_in_keras"
                            class="icon brands alt fa-github"><span class="label">GitHub Code</span></a></li>
                <h2>Save and Load Your Keras Deep Learning Models</h2>
                <p>We will discover how to save Keras models to file and load them up again to make predictions.</p>
            </article>
            <article class="thumb">
                <a href="https://miro.medium.com/max/1900/1*GjehOa513_BgpDDP6Vkw2Q.gif"
                    class="image"><img src="https://media3.giphy.com/media/Lny6Rw04nsOOc/giphy.gif" alt="" /></a>
                    <li><a href="https://github.com/Ajayprr/1_life_cycle_neural_network_models_in_keras"
                            class="icon brands alt fa-github"><span class="label">GitHub Code</span></a></li>
                <h2>Text Generation With LSTM Recurrent Neural Networks</h2>
                <p>Where to download a free corpus of text that you can use to train text generative models.
                How to frame the problem of text sequences to a recurrent neural network generative model.
                How to develop an LSTM to generate plausible text sequences for a given problem.</p>
            </article>
            <article class="thumb">
                <a href="https://miro.medium.com/max/1920/1*n-IgHZM5baBUjq0T7RYDBw.gif" class="image"><img
                        src="https://thumbs.gfycat.com/AdorableAllIchthyosaurs-max-1mb.gif" alt="" /></a>
                        <li><a href="https://github.com/Ajayprr/1_life_cycle_neural_network_models_in_keras"
                                class="icon brands alt fa-github"><span class="label">GitHub Code</span></a></li>
                <h2>Understanding Stateful LSTM Recurrent Neural Networks</h2>
                <p>How to develop a naive LSTM network for a sequence prediction problem.<br>
                How to carefully manage state through batches and features with an LSTM network.<br>
                Hot to manually manage state in an LSTM network for stateful prediction.</p>
            </article>
            <article class="thumb">
                <a href="https://thumbs.gfycat.com/EquatorialSpicyEgg-size_restricted.gif" class="image"><img
                        src="https://courses.cs.washington.edu/courses/cse473/06sp/BackProp/bnn_files/backprop_movie.gif" alt="" /></a>
                        <li><a href="https://github.com/Ajayprr/1_life_cycle_neural_network_models_in_keras"
                                class="icon brands alt fa-github"><span class="label">GitHub Code</span></a></li>
                <h2>A Gentle Introduction to Backpropagation Through Time</h2>
                <p>What Backpropagation Through Time is and how it relates to the Backpropagation training algorithm used by Multilayer
                Perceptron networks.
                The motivations that lead to the need for Truncated Backpropagation Through Time, the most widely used variant in deep
                learning for training LSTMs.
                A notation for thinking about how to configure Truncated Backpropagation Through Time and the canonical configurations
                used in research and by deep learning libraries.</p>
            </article>
            <article class="thumb">
                <a href="https://miro.medium.com/max/3840/1*4Zk-5y_XZdD3x7xk6JytGA.gif" class="image"><img
                        src="https://miro.medium.com/max/3840/1*4Zk-5y_XZdD3x7xk6JytGA.gif" alt="" /></a>
                        <li><a href="https://github.com/Ajayprr/1_life_cycle_neural_network_models_in_keras"
                                class="icon brands alt fa-github"><span class="label">GitHub Code</span></a></li>
                <h2 style="color:white;">Attention in Long Short-Term Memory RNN</h2>
                <p>The limitation of the encode-decoder architecture and the fixed-length internal representation.
                The attention mechanism to overcome the limitation that allows the network to learn where to pay attention in the input
                sequence for each item in the output sequence.
                5 applications of the attention mechanism with recurrent neural networks in domains such as text translation, speech
                recognition, and more.</p>
            </article>
            <article class="thumb">
                <a href="https://media.springernature.com/original/springer-static/image/chp%3A10.1007%2F978-3-319-59060-8_50/MediaObjects/450585_1_En_50_Fig1_HTML.gif" class="image"><img
                        src="https://sknadig.me/assets/posts/enc_dec/encoder.gif" alt="" /></a>
                        <li><a href="https://github.com/Ajayprr/1_life_cycle_neural_network_models_in_keras"
                                class="icon brands alt fa-github"><span class="label">GitHub Code</span></a></li>
                <h2>Encoder-Decoder Long Short-Term Memory Networks</h2>
                <p>The challenge of sequence-to-sequence prediction.
                The Encoder-Decoder architecture and the limitation in LSTMs that it was designed to address.
                How to implement the Encoder-Decoder LSTM model architecture in Python with Keras.</p>
            </article>
            <article class="thumb">
                <a href="https://cdn-images-1.medium.com/max/1600/1*tUhgr3m54Qc80GU2BkaOiQ.gif" class="image"><img
                        src="https://i.gifer.com/NYRT.gif" alt="" /></a>
                        <li><a href="https://github.com/Ajayprr/1_life_cycle_neural_network_models_in_keras"
                                class="icon brands alt fa-github"><span class="label">GitHub Code</span></a></li>
                <h2>Introduction to Generative Long Short-Term Memory Networks</h2>
                <p>About generative models, with a focus on generative models for text called language modeling.
                Examples of applications where LSTM Generative models have been used.
                Examples of how to model text for generative models with LSTMs.</p>
            </article>
            <article class="thumb">
                <a href="https://www.researchgate.net/profile/Christian_Omlin/publication/2627624/figure/fig4/AS:279442342596610@1443635654061/1-A-Time-Delay-Neural-Network.png" class="image"><img
                        src="https://kaleidoescape.github.io/assets/img/tdnn-contig.gif" alt="" /></a>
                        
                <h2>TimeDistributed Layer for Long Short-Term Memory Networks</h2>
                <p>How to design a one-to-one LSTM for sequence prediction.
                How to design a many-to-one LSTM for sequence prediction without the TimeDistributed Layer.
                How to design a many-to-many LSTM for sequence prediction with the TimeDistributed Layer.</p>
                <li><a href="#" class="icon brands alt fa-github"><span class="label">GitHub Code</span></a></li>
            </article>
            
        </div>

        <!-- Footer -->


    </div>

    <!-- Scripts -->
    <script src="assets1/js/jquery.min.js"></script>
    <script src="assets1/js/jquery.poptrox.min.js"></script>
    <script src="assets1/js/browser.min.js"></script>
    <script src="assets1/js/breakpoints.min.js"></script>
    <script src="assets1/js/util.js"></script>
    <script src="assets1/js/main.js"></script>

</body>

</html>